---
title: "Agents"
description: Prompted LLM chatbots that can reason around data
---

A data agent is an LLM-based agent that can manipulate and synthesize data --
the data agent and the agent configs that define them are at the heart of onyx.

Typically, to prompt engineer a _data_ agent (an agent that can reason around
data), you need a heavily manual workflow involving database schema ingestion,
LLM prompting and step chaining/iteration, and SQL query retrieval and
injection. Our `.agent.yml` files abstract away much of this complexity,
allowing you to focus more on the _logic_ of the prompted LLM rather than the
details of the code execution.

## Agent components

Specifically, in our agent `yml` files, you need to specify the following:

| Component           | Description                                                                                | Required |
| ------------------- | ------------------------------------------------------------------------------------------ | -------- |
| model               | LLM model to use (defined in [config.yml](/learn-about-onyx/config), referenced by `name`) | Required |
| system_instructions | System instructions passed to the LLM                                                      | Required |
| context             | A list of files that can be injected into `system_instructions`                            | Optional |
| output_format       | This can be either `file` or `default`                                                     | Optional |
| tools               | Tools to use, see the Tools section below                                                  | Optional |
| database            | Database to use (defined in [config.yml](/learn-about-onyx/config), referenced by `name`)  | Optional |

### Context

The context object allows for deterministic injection of files into
`system_instructions`. Primarily this is an organizational and code re-use
utility so rather than prompting your agents with long text blobs, you can
instead divide these prompts into logical sections, broken into distinct files.
Each context entry in the list requires three fields:

| Field | Description                                                                       | Example             |
| ----- | --------------------------------------------------------------------------------- | ------------------- |
| name  | Identifier for the context that will be used in system_instructions               | "anon_youtube"      |
| type  | Type of the context file (e.g., file, semantic_model)                             | "semantic_model"    |
| src   | Path to the source file relative to the project root (can be single file or list) | "data/acme.sem.yml" |

You can reference `file`-type context objects in your system_instructions using the following syntax:
`{{ context.name }}`
For objects that are of type `semantic_model`, you can access their properties using:
`{{ context.name.property }}`
For example, if you have a semantic model context named "acme", you can access its entities using:
`{{ context.acme.entities }}`

We encourage saving any pertinent SQL files and injecting these in

### Tools

To enable the LLM to flexibly accomplish a wider range of tasks, our internal chain logic is as follows:

1. We render your `system_instructions` using all retrieved queries and context.
2. We feed the rendered `system_instructions` and prompt into the LLM tool calling API.
3. We repeat steps 1 and 2 with the results of step 2 until the request specified in `system_instructions` is fulfilled.

### Database

Database information can be accessed within `system_instructions` by using the `databases` namespace, then referencing by `name`, as follows:

```
**Dataset: {{ databases.primary_database.name }}**
**Dialect: {{ databases.primary_database.dialect }}**
**Tables:**
{% for table in databases.primary_database.tables %}
{{ table }}
{% endfor %}
```

### Sample config

```yaml semantic_model.agent.yml
model: "openai-4o-mini"
context:
  - name: anon_youtube
    type: semantic_model
    src: data/acme.sem.yml
system_instructions: |
  ## Instructions
  You are a Data Analyst expert. Your role is to assist the user in generating and executing SQL queries to answer their questions.
  ### Adhere to these rules:
  - **Carefully analyze the user's question and the database schema, word by word**, to ensure the query accurately addresses the request.
  - **Use table aliases** to avoid ambiguity in queries. For example: `SELECT t1.col1, t2.col1 FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id`.
  - **Automatically execute the SQL query** without seeking additional confirmation.
  - If the SQL query results in an error, **attempt to debug, fix, and re-execute it** without requesting confirmation.
  - Ensure the SQL query adheres to the **specific dialect of the database database** being used.
  - **Return the SQL query as plain text** (not in JSON format).

  **Schema information:**
  Entities:
  {{ context.acme.entities }}
  Dimensions:
  {{ context.acme.dimensions }}
  Measures:
  {{ context.acme.measures }}

  **Dataset: {{ databases.primary_database.name }}**
  **Dialect: {{ databases.primary_database.dialect }}**
  **Tables:**
  {% for table in databases.primary_database.tables %}
  {{ table }}
  {% endfor %}

output_format: file
tools:
  - name: retrieval
    type: retrieval
    retrieval_type: all-shot
    data:
      - "."
      - "default"
  - name: execute_sql
    type: execute_sql
    database: local
```
